{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e8abb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "memory = '2g'\n",
    "pyspark_submit_args = ' --driver-memory ' + memory + ' pyspark-shell'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = pyspark_submit_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "985ea6aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/02/26 16:28:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/02/26 16:28:22 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/02/26 16:28:22 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98fb924e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-02-25 21:59:39--  https://nyc-tlc.s3.amazonaws.com/trip+data/fhvhv_tripdata_2021-02.csv\n",
      "Resolving nyc-tlc.s3.amazonaws.com (nyc-tlc.s3.amazonaws.com)... 52.216.107.188\n",
      "Connecting to nyc-tlc.s3.amazonaws.com (nyc-tlc.s3.amazonaws.com)|52.216.107.188|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 733822658 (700M) [text/csv]\n",
      "Saving to: ‘fhvhv_tripdata_2021-02.csv’\n",
      "\n",
      "fhvhv_tripdata_2021 100%[===================>] 699.83M  5.85MB/s    in 4m 20s  \n",
      "\n",
      "2022-02-25 22:03:59 (2.69 MB/s) - ‘fhvhv_tripdata_2021-02.csv’ saved [733822658/733822658]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://nyc-tlc.s3.amazonaws.com/trip+data/fhvhv_tripdata_2021-02.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1087df0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.2.1'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question 1: Version of PySpark *\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8af37212",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|hvfhs_license_num|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|           HV0003|              B02764|2021-02-01 00:10:40|2021-02-01 00:21:09|          35|          39|   null|\n",
      "|           HV0003|              B02764|2021-02-01 00:27:23|2021-02-01 00:44:01|          39|          35|   null|\n",
      "|           HV0005|              B02510|2021-02-01 00:28:38|2021-02-01 00:38:27|          39|          91|   null|\n",
      "|           HV0005|              B02510|2021-02-01 00:43:37|2021-02-01 01:23:20|          91|         228|   null|\n",
      "|           HV0003|              B02872|2021-02-01 00:08:42|2021-02-01 00:17:57|         126|         250|   null|\n",
      "|           HV0003|              B02872|2021-02-01 00:26:02|2021-02-01 00:42:51|         208|         243|   null|\n",
      "|           HV0003|              B02872|2021-02-01 00:45:50|2021-02-01 01:02:50|         243|         220|   null|\n",
      "|           HV0003|              B02764|2021-02-01 00:06:42|2021-02-01 00:31:50|          49|          37|   null|\n",
      "|           HV0003|              B02764|2021-02-01 00:34:34|2021-02-01 00:58:13|          37|          76|   null|\n",
      "|           HV0005|              B02510|2021-02-01 00:03:43|2021-02-01 00:39:37|          80|         241|   null|\n",
      "|           HV0005|              B02510|2021-02-01 00:55:36|2021-02-01 01:08:39|         174|          51|   null|\n",
      "|           HV0005|              B02510|2021-02-01 00:06:13|2021-02-01 00:33:45|         235|         129|   null|\n",
      "|           HV0005|              B02510|2021-02-01 00:42:24|2021-02-01 01:11:31|         129|         169|   null|\n",
      "|           HV0003|              B02764|2021-02-01 00:07:05|2021-02-01 00:20:53|         226|          82|   null|\n",
      "|           HV0003|              B02764|2021-02-01 00:28:56|2021-02-01 00:33:59|          82|         129|   null|\n",
      "|           HV0003|              B02764|2021-02-01 00:44:53|2021-02-01 01:07:54|           7|          79|   null|\n",
      "|           HV0003|              B02888|2021-02-01 00:17:55|2021-02-01 00:34:41|           4|         170|   null|\n",
      "|           HV0003|              B02888|2021-02-01 00:38:14|2021-02-01 00:59:20|         164|          42|   null|\n",
      "|           HV0004|              B02800|2021-02-01 00:08:04|2021-02-01 00:24:41|         237|           4|   null|\n",
      "|           HV0004|              B02800|2021-02-01 00:30:44|2021-02-01 00:41:26|         107|          45|   null|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import types\n",
    "schema = types.StructType([\n",
    "\ttypes.StructField('hvfhs_license_num',types.StringType(),True),\n",
    "\ttypes.StructField('dispatching_base_num',types.StringType(),True),\n",
    "\ttypes.StructField('pickup_datetime',types.TimestampType(),True),\n",
    "\ttypes.StructField('dropoff_datetime',types.TimestampType(),True),\n",
    "\ttypes.StructField('PULocationID',types.IntegerType(),True),\n",
    "\ttypes.StructField('DOLocationID',types.IntegerType(),True),\n",
    "\ttypes.StructField('SR_Flag',types.StringType(),True)\n",
    "])\n",
    "\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv('fhvhv_tripdata_2021-02.csv')\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "786a2d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/02/26 16:34:29 WARN MemoryManager: Total allocation exceeds 95.00% (1,813,485,955 bytes) of heap memory\n",
      "Scaling row group sizes to 96.51% for 14 writers\n",
      "22/02/26 16:34:29 WARN MemoryManager: Total allocation exceeds 95.00% (1,813,485,955 bytes) of heap memory\n",
      "Scaling row group sizes to 90.08% for 15 writers\n",
      "22/02/26 16:34:29 WARN MemoryManager: Total allocation exceeds 95.00% (1,813,485,955 bytes) of heap memory\n",
      "Scaling row group sizes to 84.45% for 16 writers\n",
      "22/02/26 16:34:32 WARN MemoryManager: Total allocation exceeds 95.00% (1,813,485,955 bytes) of heap memory\n",
      "Scaling row group sizes to 90.08% for 15 writers\n",
      "22/02/26 16:34:32 WARN MemoryManager: Total allocation exceeds 95.00% (1,813,485,955 bytes) of heap memory\n",
      "Scaling row group sizes to 96.51% for 14 writers\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# This is important to update the actual dataframe\n",
    "df = df.repartition(24)\n",
    "df.write.parquet('fhvhv/2021/02/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1c564e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 442432\r\n",
      "-rw-r--r--  1 sengopal  staff   8.4M Feb 26 16:34 part-00007-96c9c86d-9dbb-499d-a19d-b63d0dff98c1-c000.snappy.parquet\r\n",
      "-rw-r--r--  1 sengopal  staff   8.4M Feb 26 16:34 part-00002-96c9c86d-9dbb-499d-a19d-b63d0dff98c1-c000.snappy.parquet\r\n",
      "-rw-r--r--  1 sengopal  staff   8.4M Feb 26 16:34 part-00012-96c9c86d-9dbb-499d-a19d-b63d0dff98c1-c000.snappy.parquet\r\n",
      "-rw-r--r--  1 sengopal  staff   8.4M Feb 26 16:34 part-00009-96c9c86d-9dbb-499d-a19d-b63d0dff98c1-c000.snappy.parquet\r\n",
      "-rw-r--r--  1 sengopal  staff   8.4M Feb 26 16:34 part-00006-96c9c86d-9dbb-499d-a19d-b63d0dff98c1-c000.snappy.parquet\r\n",
      "-rw-r--r--  1 sengopal  staff   8.4M Feb 26 16:34 part-00004-96c9c86d-9dbb-499d-a19d-b63d0dff98c1-c000.snappy.parquet\r\n",
      "-rw-r--r--  1 sengopal  staff   8.4M Feb 26 16:34 part-00014-96c9c86d-9dbb-499d-a19d-b63d0dff98c1-c000.snappy.parquet\r\n",
      "-rw-r--r--  1 sengopal  staff   8.4M Feb 26 16:34 part-00013-96c9c86d-9dbb-499d-a19d-b63d0dff98c1-c000.snappy.parquet\r\n",
      "-rw-r--r--  1 sengopal  staff   8.4M Feb 26 16:34 part-00003-96c9c86d-9dbb-499d-a19d-b63d0dff98c1-c000.snappy.parquet\r\n",
      "-rw-r--r--  1 sengopal  staff   8.4M Feb 26 16:34 part-00005-96c9c86d-9dbb-499d-a19d-b63d0dff98c1-c000.snappy.parquet\r\n",
      "-rw-r--r--  1 sengopal  staff   8.4M Feb 26 16:34 part-00010-96c9c86d-9dbb-499d-a19d-b63d0dff98c1-c000.snappy.parquet\r\n",
      "-rw-r--r--  1 sengopal  staff   8.4M Feb 26 16:34 part-00001-96c9c86d-9dbb-499d-a19d-b63d0dff98c1-c000.snappy.parquet\r\n",
      "-rw-r--r--  1 sengopal  staff   8.4M Feb 26 16:34 part-00008-96c9c86d-9dbb-499d-a19d-b63d0dff98c1-c000.snappy.parquet\r\n",
      "-rw-r--r--  1 sengopal  staff   8.4M Feb 26 16:34 part-00000-96c9c86d-9dbb-499d-a19d-b63d0dff98c1-c000.snappy.parquet\r\n",
      "-rw-r--r--  1 sengopal  staff   8.4M Feb 26 16:34 part-00015-96c9c86d-9dbb-499d-a19d-b63d0dff98c1-c000.snappy.parquet\r\n",
      "-rw-r--r--  1 sengopal  staff   8.4M Feb 26 16:34 part-00011-96c9c86d-9dbb-499d-a19d-b63d0dff98c1-c000.snappy.parquet\r\n",
      "-rw-r--r--  1 sengopal  staff   8.4M Feb 26 16:34 part-00019-96c9c86d-9dbb-499d-a19d-b63d0dff98c1-c000.snappy.parquet\r\n",
      "-rw-r--r--  1 sengopal  staff   8.4M Feb 26 16:34 part-00017-96c9c86d-9dbb-499d-a19d-b63d0dff98c1-c000.snappy.parquet\r\n",
      "-rw-r--r--  1 sengopal  staff   8.4M Feb 26 16:34 part-00016-96c9c86d-9dbb-499d-a19d-b63d0dff98c1-c000.snappy.parquet\r\n",
      "-rw-r--r--  1 sengopal  staff   8.4M Feb 26 16:34 part-00018-96c9c86d-9dbb-499d-a19d-b63d0dff98c1-c000.snappy.parquet\r\n",
      "-rw-r--r--  1 sengopal  staff   8.4M Feb 26 16:34 part-00020-96c9c86d-9dbb-499d-a19d-b63d0dff98c1-c000.snappy.parquet\r\n",
      "-rw-r--r--  1 sengopal  staff   8.4M Feb 26 16:34 part-00022-96c9c86d-9dbb-499d-a19d-b63d0dff98c1-c000.snappy.parquet\r\n",
      "-rw-r--r--  1 sengopal  staff   8.4M Feb 26 16:34 part-00021-96c9c86d-9dbb-499d-a19d-b63d0dff98c1-c000.snappy.parquet\r\n",
      "-rw-r--r--  1 sengopal  staff   8.4M Feb 26 16:34 part-00023-96c9c86d-9dbb-499d-a19d-b63d0dff98c1-c000.snappy.parquet\r\n",
      "-rw-r--r--  1 sengopal  staff     0B Feb 26 16:34 _SUCCESS\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lrth fhvhv/2021/02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb5d2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 2: Size of HVFHW February 2021 *\n",
    "# Repartition it to 24 partitions and save it to parquet.\n",
    "# What's the size of the folder with results (in MB)?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb1a3ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11613942"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c7a9e7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fhv_2021_02 = df.registerTempTable('fhv_2021_02')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8b23f0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3: How many taxi trips were there on February 15?\n",
    "# Consider only trips that started on February 15.\n",
    "\n",
    "df_trips_feb15 = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    date_trunc('date', pickup_datetime) AS date, \n",
    "    COUNT(1) AS number_records\n",
    "FROM\n",
    "    fhv_2021_02\n",
    "WHERE   \n",
    "    date_format(pickup_datetime,'yyyy-MM-dd') = '2021-02-15'\n",
    "GROUP BY\n",
    "    1\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5ed5283a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 28:====================================================>   (15 + 1) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------+\n",
      "|date|number_records|\n",
      "+----+--------------+\n",
      "|null|        367170|\n",
      "+----+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_trips_feb15.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "09c40d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Question 4. Longest trip for each day. Now calculate the duration for each trip. \n",
    "## Trip starting on which day was the longest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8da2cd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "df_diff = df.withColumn('DiffInSeconds',col('dropoff_datetime').cast(\"long\") - col('pickup_datetime').cast(\"long\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aed1f60c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(hvfhs_license_num,StringType,true),StructField(dispatching_base_num,StringType,true),StructField(pickup_datetime,TimestampType,true),StructField(dropoff_datetime,TimestampType,true),StructField(PULocationID,IntegerType,true),StructField(DOLocationID,IntegerType,true),StructField(SR_Flag,StringType,true),StructField(DiffInSeconds,LongType,true)))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_diff.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7b7edcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fhv_2021_02_duration = df_diff.registerTempTable('fhv_2021_02_duration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3e01dfa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 48:=====================================>                  (16 + 8) / 24]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------+\n",
      "|    pickup_datetime|DiffInSeconds|\n",
      "+-------------------+-------------+\n",
      "|2021-02-01 00:00:16|          819|\n",
      "|2021-02-01 00:01:07|          918|\n",
      "|2021-02-01 00:01:11|         1201|\n",
      "|2021-02-01 00:01:17|         1040|\n",
      "|2021-02-01 00:01:36|          564|\n",
      "|2021-02-01 00:02:09|          608|\n",
      "|2021-02-01 00:02:20|         1604|\n",
      "|2021-02-01 00:02:31|          698|\n",
      "|2021-02-01 00:03:14|          724|\n",
      "|2021-02-01 00:03:45|          971|\n",
      "|2021-02-01 00:03:58|         1328|\n",
      "|2021-02-01 00:04:05|         1180|\n",
      "|2021-02-01 00:04:25|          507|\n",
      "|2021-02-01 00:04:49|         1139|\n",
      "|2021-02-01 00:05:00|          705|\n",
      "|2021-02-01 00:05:04|         1467|\n",
      "|2021-02-01 00:05:34|         1019|\n",
      "|2021-02-01 00:05:53|         1864|\n",
      "|2021-02-01 00:06:09|         1436|\n",
      "|2021-02-01 00:07:08|         1144|\n",
      "+-------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_trips_longest = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    pickup_datetime, MAX(DiffInSeconds)\n",
    "FROM\n",
    "    (SELECT *, MAX(DiffInSeconds) OVER (PARTITION BY pickup_datetime) AS maxDiffInSeconds FROM fhv_2021_02_duration) M WHERE DiffInSeconds = maxDiffInSeconds\n",
    "\"\"\")\n",
    "\n",
    "df_trips_longest.show()\n",
    "\n",
    "# \"SELECT A, B FROM (SELECT *, MAX(B) OVER (PARTITION BY A) AS maxB FROM table) M WHERE B = maxB\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bbe1906a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(pickup_datetime=datetime.datetime(2021, 2, 11, 13, 40, 44), DiffInSeconds=75540),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 2, 17, 15, 54, 53), DiffInSeconds=57221),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 2, 20, 12, 8, 15), DiffInSeconds=44039),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 2, 3, 20, 24, 25), DiffInSeconds=40653),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 2, 19, 23, 17, 44), DiffInSeconds=37577)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trips_longest = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    pickup_datetime, DiffInSeconds\n",
    "FROM\n",
    "    fhv_2021_02_duration ORDER BY DiffInSeconds DESC\n",
    "\"\"\")\n",
    "df_trips_longest.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "aea042c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'show'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/8_/5jrrzcc913ld81kz2_vhx2jr0000gq/T/ipykernel_34352/4194280670.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf_collect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_diff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DiffInSeconds'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'B'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_collect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'show'"
     ]
    }
   ],
   "source": [
    "df_collect = df_diff.agg(max('DiffInSeconds').alias('B')).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d98ed1a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(B=75540)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46765c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 5. Most frequent dispatching_base_num\n",
    "# Now find the most frequently occurring dispatching_base_num in this dataset.\n",
    "# How many stages this spark job has?\n",
    "# Note: the answer may depend on how you write the query, so there are multiple correct answers. Select the one you have.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5e3a23cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(dispatching_base_num='B02510', number_records=3233664),\n",
       " Row(dispatching_base_num='B02764', number_records=965568),\n",
       " Row(dispatching_base_num='B02872', number_records=882689),\n",
       " Row(dispatching_base_num='B02875', number_records=685390),\n",
       " Row(dispatching_base_num='B02765', number_records=559768)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trips_dispatch_num = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    dispatching_base_num,\n",
    "    COUNT(1) AS number_records\n",
    "FROM\n",
    "    fhv_2021_02\n",
    "GROUP BY\n",
    "    1\n",
    "ORDER BY number_records DESC  \n",
    "\"\"\")\n",
    "df_trips_dispatch_num.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e6d3ffbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(hvfhs_license_num='HV0003', dispatching_base_num='B02764', pickup_datetime=datetime.datetime(2021, 2, 3, 11, 15, 45), dropoff_datetime=datetime.datetime(2021, 2, 3, 11, 24, 35), PULocationID=75, DOLocationID=263, SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02865', pickup_datetime=datetime.datetime(2021, 2, 1, 5, 49, 35), dropoff_datetime=datetime.datetime(2021, 2, 1, 6, 9, 19), PULocationID=62, DOLocationID=65, SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0005', dispatching_base_num='B02510', pickup_datetime=datetime.datetime(2021, 2, 2, 8, 20, 19), dropoff_datetime=datetime.datetime(2021, 2, 2, 8, 30, 8), PULocationID=208, DOLocationID=242, SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02875', pickup_datetime=datetime.datetime(2021, 2, 2, 15, 22, 24), dropoff_datetime=datetime.datetime(2021, 2, 2, 15, 33, 26), PULocationID=14, DOLocationID=14, SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0005', dispatching_base_num='B02510', pickup_datetime=datetime.datetime(2021, 2, 1, 0, 7, 2), dropoff_datetime=datetime.datetime(2021, 2, 1, 0, 53, 7), PULocationID=37, DOLocationID=201, SR_Flag=None)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "af9aeac8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(LocationID='1', Borough='EWR', Zone='Newark Airport', service_zone='EWR'),\n",
       " Row(LocationID='2', Borough='Queens', Zone='Jamaica Bay', service_zone='Boro Zone'),\n",
       " Row(LocationID='3', Borough='Bronx', Zone='Allerton/Pelham Gardens', service_zone='Boro Zone'),\n",
       " Row(LocationID='4', Borough='Manhattan', Zone='Alphabet City', service_zone='Yellow Zone'),\n",
       " Row(LocationID='5', Borough='Staten Island', Zone='Arden Heights', service_zone='Boro Zone')]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_zones = spark.read.parquet('zones/')\n",
    "df_zones.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5f5ef4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0dd48010",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(hvfhs_license_num='HV0003', dispatching_base_num='B02879', pickup_datetime=datetime.datetime(2021, 2, 2, 21, 42, 58), dropoff_datetime=datetime.datetime(2021, 2, 2, 22, 8, 4), PULocationID=31, DOLocationID=4, SR_Flag=None, PU_Borough='Bronx', PU_Zone='Bronx Park', PU_service_zone='Boro Zone')]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tmp = df.join(df_zones, df.PULocationID == df_zones.LocationID, how='outer')\n",
    "df_tmp = df_tmp.drop('LocationID')\n",
    "df_tmp = df_tmp.withColumnRenamed('Borough', 'PU_Borough').withColumnRenamed('Zone', 'PU_Zone').withColumnRenamed('service_zone', 'PU_service_zone')\n",
    "df_tmp.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c83e0bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(hvfhs_license_num='HV0003', dispatching_base_num='B02764', pickup_datetime=datetime.datetime(2021, 2, 17, 22, 10, 1), dropoff_datetime=datetime.datetime(2021, 2, 17, 22, 15, 17), PULocationID=31, DOLocationID=31, SR_Flag=None, PU_Borough='Bronx', PU_Zone='Bronx Park', PU_service_zone='Boro Zone', DO_Borough='Bronx', DO_Zone='Bronx Park', DO_service_zone='Boro Zone')]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tmp = df_tmp.join(df_zones, df_tmp.DOLocationID == df_zones.LocationID, how='outer')\n",
    "df_tmp = df_tmp.drop('LocationID')\n",
    "df_with_zones = df_tmp.withColumnRenamed('Borough', 'DO_Borough').withColumnRenamed('Zone', 'DO_Zone').withColumnRenamed('service_zone', 'DO_service_zone')\n",
    "df_with_zones.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "71df82b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(hvfhs_license_num='HV0003', dispatching_base_num='B02764', pickup_datetime=datetime.datetime(2021, 2, 17, 22, 10, 1), dropoff_datetime=datetime.datetime(2021, 2, 17, 22, 15, 17), PULocationID=31, DOLocationID=31, SR_Flag=None, PU_Borough='Bronx', PU_Zone='Bronx Park', PU_service_zone='Boro Zone', DO_Borough='Bronx', DO_Zone='Bronx Park', DO_service_zone='Boro Zone', trip_zone='Bronx Park / Bronx Park'),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02395', pickup_datetime=datetime.datetime(2021, 2, 28, 15, 42, 59), dropoff_datetime=datetime.datetime(2021, 2, 28, 15, 49, 9), PULocationID=31, DOLocationID=31, SR_Flag=None, PU_Borough='Bronx', PU_Zone='Bronx Park', PU_service_zone='Boro Zone', DO_Borough='Bronx', DO_Zone='Bronx Park', DO_service_zone='Boro Zone', trip_zone='Bronx Park / Bronx Park'),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02864', pickup_datetime=datetime.datetime(2021, 2, 17, 14, 54, 50), dropoff_datetime=datetime.datetime(2021, 2, 17, 14, 58, 58), PULocationID=31, DOLocationID=31, SR_Flag=None, PU_Borough='Bronx', PU_Zone='Bronx Park', PU_service_zone='Boro Zone', DO_Borough='Bronx', DO_Zone='Bronx Park', DO_service_zone='Boro Zone', trip_zone='Bronx Park / Bronx Park'),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02764', pickup_datetime=datetime.datetime(2021, 2, 18, 19, 42, 9), dropoff_datetime=datetime.datetime(2021, 2, 18, 19, 55, 10), PULocationID=31, DOLocationID=31, SR_Flag=None, PU_Borough='Bronx', PU_Zone='Bronx Park', PU_service_zone='Boro Zone', DO_Borough='Bronx', DO_Zone='Bronx Park', DO_service_zone='Boro Zone', trip_zone='Bronx Park / Bronx Park'),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02876', pickup_datetime=datetime.datetime(2021, 2, 26, 19, 20, 23), dropoff_datetime=datetime.datetime(2021, 2, 26, 19, 27, 17), PULocationID=31, DOLocationID=31, SR_Flag=None, PU_Borough='Bronx', PU_Zone='Bronx Park', PU_service_zone='Boro Zone', DO_Borough='Bronx', DO_Zone='Bronx Park', DO_service_zone='Boro Zone', trip_zone='Bronx Park / Bronx Park'),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02875', pickup_datetime=datetime.datetime(2021, 2, 3, 2, 7, 49), dropoff_datetime=datetime.datetime(2021, 2, 3, 2, 11, 4), PULocationID=31, DOLocationID=31, SR_Flag=None, PU_Borough='Bronx', PU_Zone='Bronx Park', PU_service_zone='Boro Zone', DO_Borough='Bronx', DO_Zone='Bronx Park', DO_service_zone='Boro Zone', trip_zone='Bronx Park / Bronx Park'),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02835', pickup_datetime=datetime.datetime(2021, 2, 21, 21, 56, 42), dropoff_datetime=datetime.datetime(2021, 2, 21, 22, 42, 41), PULocationID=31, DOLocationID=31, SR_Flag=None, PU_Borough='Bronx', PU_Zone='Bronx Park', PU_service_zone='Boro Zone', DO_Borough='Bronx', DO_Zone='Bronx Park', DO_service_zone='Boro Zone', trip_zone='Bronx Park / Bronx Park'),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02887', pickup_datetime=datetime.datetime(2021, 2, 3, 6, 23, 57), dropoff_datetime=datetime.datetime(2021, 2, 3, 6, 31, 36), PULocationID=31, DOLocationID=31, SR_Flag=None, PU_Borough='Bronx', PU_Zone='Bronx Park', PU_service_zone='Boro Zone', DO_Borough='Bronx', DO_Zone='Bronx Park', DO_service_zone='Boro Zone', trip_zone='Bronx Park / Bronx Park'),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02872', pickup_datetime=datetime.datetime(2021, 2, 14, 1, 29, 45), dropoff_datetime=datetime.datetime(2021, 2, 14, 1, 34, 24), PULocationID=31, DOLocationID=31, SR_Flag=None, PU_Borough='Bronx', PU_Zone='Bronx Park', PU_service_zone='Boro Zone', DO_Borough='Bronx', DO_Zone='Bronx Park', DO_service_zone='Boro Zone', trip_zone='Bronx Park / Bronx Park'),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02875', pickup_datetime=datetime.datetime(2021, 2, 10, 6, 29, 20), dropoff_datetime=datetime.datetime(2021, 2, 10, 6, 34, 57), PULocationID=31, DOLocationID=31, SR_Flag=None, PU_Borough='Bronx', PU_Zone='Bronx Park', PU_service_zone='Boro Zone', DO_Borough='Bronx', DO_Zone='Bronx Park', DO_service_zone='Boro Zone', trip_zone='Bronx Park / Bronx Park')]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question 6. Most common locations pair. Find the most common pickup-dropoff pair.\n",
    "# For example: \"Jamaica Bay / Clinton East\"\n",
    "\n",
    "# Enter two zone names separated by a slash\n",
    "\n",
    "# If any of the zone names are unknown (missing), use \"Unknown\". For example, \"Unknown / Clinton East\".\n",
    "\n",
    "from pyspark.sql.functions import coalesce\n",
    "\n",
    "df_zone_concat = df_with_zones.withColumn(\"trip_zone\",\\\n",
    "    concat(coalesce(df_with_zones.PU_Zone, lit('Unknown')), lit(' / '), coalesce(df_with_zones.DO_Zone, lit('Unknown'))))\n",
    "\n",
    "df_zone_concat.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "16bee1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fhv_with_zones = df_zone_concat.registerTempTable('fhv_with_zones')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "044ece1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(trip_zone='East New York / East New York', number_trips=45041),\n",
       " Row(trip_zone='Borough Park / Borough Park', number_trips=37329),\n",
       " Row(trip_zone='Canarsie / Canarsie', number_trips=28026),\n",
       " Row(trip_zone='Crown Heights North / Crown Heights North', number_trips=25976),\n",
       " Row(trip_zone='Bay Ridge / Bay Ridge', number_trips=17934)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trips_common = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    trip_zone,\n",
    "    COUNT(1) AS number_trips\n",
    "FROM\n",
    "    fhv_with_zones\n",
    "GROUP BY\n",
    "    1\n",
    "ORDER BY number_trips DESC  \n",
    "\"\"\")\n",
    "\n",
    "df_trips_common.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204d4351",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
